{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic Tensor Networks\n",
    "This notebook goes through some key concepts in logic tensor networks. Some of the content is modified from the excellent tutorials and examples available at https://github.com/tommasocarraro/LTNtorch. We will start off by going through the basics of constants, predicates, variables, and training, and finish up with an exercise implementing the running example of friends and movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Constants are vectors, matrices, or tensors. To define a vector constant of value $(3.4, 6.2)$, we first create a PyTorch tensor, and wrap that in a `ltn.Constant` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_tensor = torch.tensor([3.4, 6.2])\n",
    "c1 = ltn.Constant(c1_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simply do that in one step, or with higher rank tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = ltn.Constant(torch.tensor([3.4, 6.2]))\n",
    "c2 = ltn.Constant(torch.tensor([[3.4, 7.6], [6.2, 4.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a constant with value $(1.3, 9.8)$ and one with value $\\begin{pmatrix}\n",
    "4.5 & 8 & -2.5\\\\\n",
    "2 & -1.5 & 2.1\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = ltn.Constant(torch.tensor([1.3, 9.8]))\n",
    "\n",
    "c4 = ltn.Constant(torch.tensor([[4.5, 8, -2.5],[2, -1.5, 2.1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime we will want to be able to train the constants (e.g. if we are learning embeddings). We can set constants to be trainable using the argument `trainable=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 =  ltn.Constant(torch.tensor([3.4, 6.2]), trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a constant with value $(1.3, 9.8)$ and one with value $\\begin{pmatrix}\n",
    "4.5 & 8 & -2.5\\\\\n",
    "2 & -1.5 & 2.1\n",
    "\\end{pmatrix}$ which are both set to be trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the value of a constant using the `value` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.4000, 6.2000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(c1.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the value of `c2`. How is it different from `c1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.4000, 7.6000],\n",
      "        [6.2000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(c2.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicates\n",
    "\n",
    "The predicates we use will most often be feedforward neural networks. We can build these easily in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a feedforward neural network by creating a class\n",
    "# that extends `torch.nn.Module`\n",
    "\n",
    "class ModelP2(torch.nn.Module):\n",
    "    \"\"\"For more info on how to use torch.nn.Module:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
    "    # define how to initialise the network\n",
    "    def __init__(self):\n",
    "        super(ModelP2, self).__init__()\n",
    "        # We set up the activation functions\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        # We define the weight matrices\n",
    "        self.dense1 = torch.nn.Linear(4, 5)\n",
    "        self.dense2 = torch.nn.Linear(5, 1) # returns one value in [0,1]\n",
    "    \n",
    "    # Now we define the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.dense1(x))\n",
    "        return self.sigmoid(self.dense2(x))\n",
    "\n",
    "# Set up an object as an instance of the class\n",
    "modelP2 = ModelP2()\n",
    "# Finally, wrap it with `ltn.Predicate`\n",
    "P2 = ltn.Predicate(model=modelP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predicate called P3, using a feedforward neural network, that takes in a 5 dimensional input, has one hidden layer of size 8, another hidden layer of size 10, and output of size 1. Activation functions at the hidden layers are  ELU, and at the output layer is a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelP3(torch.nn.Module):\n",
    "    # define how to initialise the network\n",
    "    def __init__(self):\n",
    "        super(ModelP3, self).__init__()\n",
    "        # We set up the activation functions\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        # We define the weight matrices\n",
    "        self.dense1 = torch.nn.Linear(5, 8)\n",
    "        self.dense2 = torch.nn.Linear(8, 10)\n",
    "        self.dense3 = torch.nn.Linear(10, 1)\n",
    "        # Set up the activation functions\n",
    "        \n",
    "        # Define the weight matrices\n",
    "        \n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.dense1(x))\n",
    "        x = self.elu(self.dense2(x))\n",
    "        return self.sigmoid(self.dense3(x))\n",
    "    \n",
    "# Set up an object as an instance of the class\n",
    "modelP3 = ModelP3()\n",
    "# Finally, wrap it with `ltn.Predicate`\n",
    "P3 = ltn.Predicate(model=modelP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Variables are lists of values. They can be implemented as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ltn.Variable('x', torch.randn((10, 2)))\n",
    "y = ltn.Variable('y', torch.randn((5, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable called `z` containing 6 randomly initialised vectors of size 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ltn.Variable('z', torch.randn(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the predicate `P2` to the variable and print the output. What do you notice about the shape of the output? Does it make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LTNObject(value=tensor([0.5251, 0.6094, 0.4433, 0.4807, 0.5444, 0.6796],\n",
       "       grad_fn=<ViewBackward0>), free_vars=['z'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P2(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create variables as stacks of trainable constants. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = {}\n",
    "for i, zi in enumerate(z.value):\n",
    "    var_dict[i] = ltn.Constant(zi, trainable=True)\n",
    "    \n",
    "var_x = ltn.Variable(\"var_x\", torch.stack([i.value for i in var_dict.values()]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectives\n",
    "\n",
    "Fuzzy operators in LTN have all been implemented. We can construct connectives as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you construct a set of connectives that use the Lukasiewicz operators? Hint: look at documentation here: https://logictensornetworks.github.io/LTNtorch/fuzzy_ops.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning embeddings with LTNs\n",
    "\n",
    "We are going to build an extended model of the users/film example we have been looking at in class.\n",
    "\n",
    "We will have 8 users: $a$, $b$, $c$, $d$, $e$, $f$, $g$, $h$.\n",
    "\n",
    "and 4 films: $j$, $k$, $l$, $m$.\n",
    "\n",
    "Our **domains** are $people$ and $films$.\n",
    "\n",
    "**Variables** are $x$, with domain $people$, $y$ with domain $people$ and $u$ with domain $films$\n",
    "\n",
    "We will have predicates $F(x,y)$ for *friends* and $L(x, u)$ for *likes*\n",
    "\n",
    "Question: What is the input domain of $F$? What is the input domain of $L$?\n",
    "\n",
    "**Axioms**\n",
    "$\\mathcal{F} = \\{(a, b), (a, c), (b,d), (c, d), (e, f), (e, g), (e, h), (f, h)\\}$\n",
    "\n",
    "$\\mathcal{L} = \\{(a, j), (a, l),  (b, l), (c, l), (e, k), (f, k), (g, k), (g, m), (h, m)\\}$\n",
    "\n",
    "- $F(x, y)$ for $(x, y) \\in \\mathcal{F}$\n",
    "- $\\neg F(x, y)$ for $(x, y) \\not\\in \\mathcal{F}, u < v$\n",
    "- $L(x, u)$ for $(x, u) \\in \\mathcal{L}$\n",
    "- $\\neg L(c, j)$, $\\neg L(b, m)$, $\\neg L(c, m)$, $\\neg L(h, k)$\n",
    "- $\\forall x \\neg F(x, x)$\n",
    "- $\\forall xyu (F(x, y) \\land L(x, u)) \\implies L(y, u)$\n",
    "\n",
    "We can see this is not strictly logically satisfiable, since we have $F(a, c)$, $L(a, j)$, but also $\\neg L(c, j)$\n",
    "\n",
    "#### Grounding\n",
    "- $\\mathcal{G}(people) = \\mathbb{R}^5$\n",
    "- $\\mathcal{G}(films) = \\mathbb{R}^3$\n",
    "- $\\mathcal{G}(a) = \\vec{a},..., \\mathcal{G}(h)= \\vec{h} \\in \\mathbb{R}^5$\n",
    "- $\\mathcal{G}(j) = \\vec{j},..., \\mathcal{G}(m)= \\vec{m} \\in \\mathbb{R}^3$\n",
    "- $\\mathcal{G}(x) = \\mathcal{G}(y)= [\\vec{a},...,\\vec{h}]$\n",
    "- $\\mathcal{G}(u) = [\\vec{k},...,\\vec{m}]$\n",
    "- $\\mathcal{G}(F)$ is a function (feedforward neural network) from $x, y$ to $[0, 1]$. This is the truth value for whether $x$ and $y$ are friends.\n",
    "- $\\mathcal{G}(L)$ is a function (feedforward neural network) from $x, u$ to $[0, 1]$. This is the truth value for whether $x$ likes film $u$.\n",
    "\n",
    "#### Data\n",
    "We will start off by randomly initializing vectors for the people and the movies. \n",
    "\n",
    "We will then go on to define the knowledge we have about who is friends with who and who likes which movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltn\n",
    "import torch\n",
    "\n",
    "# Specify values for the dimensions of the people embeddings and the movie embeddings\n",
    "\n",
    "\n",
    "# Initialize a dictionary of the form {'a': trainable ltn constant, ...} for each of the people\n",
    "\n",
    "# Initialize a dictionary of the form {'j': trainable ltn constant, ...} for each of the movies\n",
    "\n",
    "# For each of 'friends' and 'movies', initialize a list of tuples of strings that specify the relations\n",
    "# 'friends' and 'movies'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2601511706.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [61]\u001b[0;36m\u001b[0m\n\u001b[0;31m    class ModelL(torch.nn.Module):\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# We define the predicates F and L as feedforward neural networks\n",
    "class ModelF(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelF, self).__init__()\n",
    "        # Specify ELU and sigmoid activation functions\n",
    "\n",
    "        # Specify 3 layers, one with input dimension suitable for the 'friends' predicate\n",
    "        # and output dim 16, one 16 to 16, and one 16 to 1\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # Specify the forward pass with ELU on the hidden layers and sigmoid on the output\n",
    "    \n",
    "\n",
    "class ModelL(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelL, self).__init__()\n",
    "        # Specify ELU and sigmoid activation functions\n",
    "\n",
    "        # Specify 3 layers, one with input dimension suitable for the 'likes' predicate\n",
    "        # and output dim 16, one 16 to 16, and one 16 to 1\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # Specify the forward pass with ELU on the hidden layers and sigmoid on the output\n",
    "\n",
    "# Wrap the models in ltn.Predicate\n",
    "\n",
    "# Define connectives, quantifiers, and SatAgg using the product configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to modify the vectors  in *people* and *movies*, and the parameters of the predicates, so that the axioms in the knowledge based are maximally satisfied.\n",
    "\n",
    "We can also define some queries. For example we can ask whether if two people like the same movie, then they are friends. How would you write this out?\n",
    "\n",
    "$\\phi_1 = \\forall x, y, u (L(x, u) \\land L(y, u)) \\implies F(x, y)$\n",
    "\n",
    "Do you predict that the following will have a high or a low truth value in this knowledge base after training?\n",
    "\n",
    "$\\phi_2 = \\forall x, y, u (F(x, y) \\land \\neg L(x, u)) \\implies L(y, u)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2469519695.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [62]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def phi2():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# this function returns the satisfaction level of the logical formula phi1\n",
    "def phi1():\n",
    "    # Create variables p, q, and r and initialize with the values from 'people' and 'movies'\n",
    "\n",
    "    # Return the truth value of phi1\n",
    "\n",
    "# this function returns the satisfaction level of the logical formula phi2\n",
    "def phi2():\n",
    "    # Create variables p, q, and r and initialize with the values from 'people' and 'movies'\n",
    "\n",
    "    # Return the truth value of phi1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the training loop to train the embeddings for `people` and `movies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We have to optimize the parameters of the three predicates and also of the embeddings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(L\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m      3\u001b[0m             [i\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m people\u001b[38;5;241m.\u001b[39mvalues()] \u001b[38;5;241m+\u001b[39m [i\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m movies\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set up a training loop for 1000 epochs\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# We have to optimize the parameters of the three predicates and also of the embeddings\n",
    "params = list(F.parameters()) + list(L.parameters()) +\\\n",
    "            [i.value for i in people.values()] + [i.value for i in movies.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set up a training loop for 1000 epochs\n",
    "for epoch in range(1000):\n",
    "    # set a variable p_exist to be 1 upto 200 epochs and 6 thereafter\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # create variables x_, y_, and z_, grounded with values from the `people` dictionary\n",
    "    \"\"\"\n",
    "    NOTE: we update the embeddings at each step\n",
    "        -> we should re-compute the variables.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    sat_agg = SatAgg(\n",
    "\n",
    "        \n",
    "        #Axioms about friends\n",
    " \n",
    "        # Likes:\n",
    "\n",
    "        # friendship is anti-reflexive (set p=5)\n",
    "\n",
    "        # friendship is symmetric (set p=5)\n",
    "\n",
    "        # everyone has a friend\n",
    "\n",
    "        # Friends like similar movies\n",
    "    )\n",
    "    \n",
    "    loss = 1. - sat_agg\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # we print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Phi1 Sat %.3f | Phi2 Sat %.3f\" % (epoch, loss,\n",
    "                    sat_agg, phi1(), phi2()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing results\n",
    "\n",
    "We wil now plot the  starting state and the state after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display options for pandas\n",
    "pd.options.display.max_rows=999\n",
    "pd.options.display.max_columns=999\n",
    "pd.set_option('display.width',1000)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Heatmap function\n",
    "def plt_heatmap(df, vmin=None, vmax=None):\n",
    "    plt.pcolor(df, vmin=vmin, vmax=vmax)\n",
    "    plt.yticks(np.arange(0.5,len(df.index),1),df.index)\n",
    "    plt.xticks(np.arange(0.5,len(df.columns),1),df.columns)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe holding facts about friends before training. Set the value to 1 if x< y and (x, y) is in friends, \n",
    "# 0 if x<y and (x, y) not in friends, and nan otherwise \n",
    "\n",
    "\n",
    "# Create a dataframe holding facts about likes before training. Set the value to 1 if (x, y) is in likes, \n",
    "# and nan otherwise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables p, q, and r with the values from people for p and q and movies for r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predicate F on variables p and q and set up a dataframe with the resulting value\n",
    "\n",
    "\n",
    "# Call the predicate L on variables p and r and set up a dataframe with the resulting value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the facts about friends as a heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the facts about likes as a heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the truth values after training as a heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the truth values for likes after training as a heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
